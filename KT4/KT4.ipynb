{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Импорты",
   "id": "2b35afe3f1f53b13"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-11-08T13:52:05.694822Z",
     "start_time": "2024-11-08T13:52:05.552155Z"
    }
   },
   "source": "import pytest",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Вариант 2\n",
    "1. А четное-ли число?"
   ],
   "id": "bdc82e8619778f85"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-08T13:52:05.739934Z",
     "start_time": "2024-11-08T13:52:05.726934Z"
    }
   },
   "cell_type": "code",
   "source": [
    "%%file is_even.py\n",
    "\n",
    "def is_even(number):\n",
    "    if number % 2 == 0:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "print(is_even(5))"
   ],
   "id": "ab0b3917f5b1d7a4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting is_even.py\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-08T13:52:06.109896Z",
     "start_time": "2024-11-08T13:52:05.869975Z"
    }
   },
   "cell_type": "code",
   "source": "!python is_even.py",
   "id": "6bd3133d8a453346",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Тестим проверку четности числа",
   "id": "33ac22807f7a717"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-08T13:52:06.216070Z",
     "start_time": "2024-11-08T13:52:06.199338Z"
    }
   },
   "cell_type": "code",
   "source": [
    "%%file test_is_even.py\n",
    "from KT4.is_even import is_even\n",
    "import pytest\n",
    "\n",
    "@pytest.mark.parametrize(\"number, expected\", [(2, True), (3, False), (4, True)])\n",
    "def test_is_even(number, expected):\n",
    "    assert is_even(number) == expected"
   ],
   "id": "250123a81c68dd17",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting test_is_even.py\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-08T13:52:07.811941Z",
     "start_time": "2024-11-08T13:52:06.278180Z"
    }
   },
   "cell_type": "code",
   "source": "!pytest test_is_even.py",
   "id": "a60c2e19b691da78",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m============================= test session starts =============================\u001B[0m\n",
      "platform win32 -- Python 3.9.7, pytest-8.3.3, pluggy-1.5.0\n",
      "rootdir: C:\\Users\\acva0\\Projects\\PycharmProjects\\Pytesting\\KT4\n",
      "plugins: anyio-4.6.2.post1, cov-5.0.0, sugar-1.0.0\n",
      "collected 3 items\n",
      "\n",
      "test_is_even.py \u001B[32m.\u001B[0m\u001B[32m.\u001B[0m\u001B[32m.\u001B[0m\u001B[32m                                                      [100%]\u001B[0m\n",
      "\n",
      "\u001B[32m============================== \u001B[32m\u001B[1m3 passed\u001B[0m\u001B[32m in 0.08s\u001B[0m\u001B[32m ==============================\u001B[0m\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 2. Считаем площадь прямоугольника",
   "id": "93efa85272741c9a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-08T13:52:07.842795Z",
     "start_time": "2024-11-08T13:52:07.831754Z"
    }
   },
   "cell_type": "code",
   "source": [
    "%%file calculate_area.py\n",
    "\n",
    "def calculate_area(length, width):\n",
    "    return length * width\n",
    "\n",
    "print(calculate_area(4, 2))"
   ],
   "id": "dd41814b45aafdce",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting calculate_area.py\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-08T13:52:08.013952Z",
     "start_time": "2024-11-08T13:52:07.890695Z"
    }
   },
   "cell_type": "code",
   "source": "!python calculate_area.py",
   "id": "ebc99cfdd5e3a092",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Тестим подсчет площади прямоугольника ",
   "id": "515090010b2d3c2a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-08T13:52:08.060172Z",
     "start_time": "2024-11-08T13:52:08.043412Z"
    }
   },
   "cell_type": "code",
   "source": [
    "%%file test_calculate_area.py\n",
    "from KT4.calculate_area import calculate_area\n",
    "import pytest\n",
    "\n",
    "@pytest.mark.parametrize(\"length, width, expected\", [(2, 4, 8), (5, 3, 15), (15, 10, 150)])\n",
    "def test_calculate_area(length, width, expected):\n",
    "    assert calculate_area(length, width) == expected"
   ],
   "id": "9f976e3968aaaa78",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting test_calculate_area.py\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-08T13:52:08.831004Z",
     "start_time": "2024-11-08T13:52:08.101925Z"
    }
   },
   "cell_type": "code",
   "source": "!pytest test_calculate_area.py",
   "id": "7fe987fe9ff0b3cb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m============================= test session starts =============================\u001B[0m\n",
      "platform win32 -- Python 3.9.7, pytest-8.3.3, pluggy-1.5.0\n",
      "rootdir: C:\\Users\\acva0\\Projects\\PycharmProjects\\Pytesting\\KT4\n",
      "plugins: anyio-4.6.2.post1, cov-5.0.0, sugar-1.0.0\n",
      "collected 3 items\n",
      "\n",
      "test_calculate_area.py \u001B[32m.\u001B[0m\u001B[32m.\u001B[0m\u001B[32m.\u001B[0m\u001B[32m                                               [100%]\u001B[0m\n",
      "\n",
      "\u001B[32m============================== \u001B[32m\u001B[1m3 passed\u001B[0m\u001B[32m in 0.05s\u001B[0m\u001B[32m ==============================\u001B[0m\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 3. Какой это треугольник?",
   "id": "6c086848b71851cb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-08T13:52:08.893550Z",
     "start_time": "2024-11-08T13:52:08.880476Z"
    }
   },
   "cell_type": "code",
   "source": [
    "%%file classify_triangle.py\n",
    "\n",
    "def classify_triangle(a, b, c):\n",
    "    if a + b <= c or a + c <= b or b + c <= a:\n",
    "        return \"Подозрительный какой-то...\"\n",
    "    \n",
    "    if a == b == c:\n",
    "        return \"Равносторонний\"\n",
    "    elif a == b or a == c or b == c:\n",
    "        return \"Равнобедренный\"\n",
    "    else:\n",
    "        return \"Разносторонний\"\n",
    "\n",
    "print(classify_triangle(2, 4, 7))"
   ],
   "id": "c70446d361806184",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting classify_triangle.py\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-08T13:52:09.080222Z",
     "start_time": "2024-11-08T13:52:08.937386Z"
    }
   },
   "cell_type": "code",
   "source": "!python classify_triangle.py",
   "id": "6fbe40a2e6b9a3f8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Подозрительный какой-то...\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Тестим определение типа треугольника",
   "id": "2822cb0bb3be11e0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-08T13:52:09.110936Z",
     "start_time": "2024-11-08T13:52:09.103577Z"
    }
   },
   "cell_type": "code",
   "source": [
    "%%file test_classify_triangle.py\n",
    "from KT4.classify_triangle import classify_triangle\n",
    "import pytest\n",
    "\n",
    "@pytest.mark.parametrize(\"a, b, c, expected\", [(2, 2, 2, \"Равносторонний\"), (5, 5, 6, \"Равнобедренный\"), (4, 5, 6, \"Разносторонний\"), (11, 5, 5, \"Подозрительный какой-то...\")])\n",
    "def test_classify_triangle(a, b, c, expected):\n",
    "    assert classify_triangle(a, b, c) == expected"
   ],
   "id": "825831c49794eff4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting test_classify_triangle.py\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-08T13:52:09.879695Z",
     "start_time": "2024-11-08T13:52:09.159451Z"
    }
   },
   "cell_type": "code",
   "source": "!pytest test_classify_triangle.py",
   "id": "8c5cbcd91c34c30c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m============================= test session starts =============================\u001B[0m\n",
      "platform win32 -- Python 3.9.7, pytest-8.3.3, pluggy-1.5.0\n",
      "rootdir: C:\\Users\\acva0\\Projects\\PycharmProjects\\Pytesting\\KT4\n",
      "plugins: anyio-4.6.2.post1, cov-5.0.0, sugar-1.0.0\n",
      "collected 4 items\n",
      "\n",
      "test_classify_triangle.py \u001B[32m.\u001B[0m\u001B[32m.\u001B[0m\u001B[32m.\u001B[0m\u001B[32m.\u001B[0m\u001B[32m                                           [100%]\u001B[0m\n",
      "\n",
      "\u001B[32m============================== \u001B[32m\u001B[1m4 passed\u001B[0m\u001B[32m in 0.05s\u001B[0m\u001B[32m ==============================\u001B[0m\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 4. Тестим апгрейд подсчета среднего",
   "id": "ad77dd6ab115d17d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-08T13:52:09.910057Z",
     "start_time": "2024-11-08T13:52:09.898415Z"
    }
   },
   "cell_type": "code",
   "source": [
    "%%file test_upgrade_KT2.py\n",
    "import pytest\n",
    "import pandas as pd\n",
    "\n",
    "@pytest.mark.parametrize(\"test1, test2, test3, test4, final, expected\", [(10, 20, 30, 40, 50, 30), (21, 35, 42, 13, 8, 20), (24,68, 39, 81, 97, 60)])\n",
    "def test_upgrade_average_grade(test1, test2, test3, test4, final, expected):\n",
    "    df = pd.DataFrame({'Test1': [test1], 'Test2': [test2], 'Test3': [test3], 'Test4': [test4], 'Final': [final]})\n",
    "    grades = df[['Test1', 'Test2', 'Test3', 'Test4', 'Final']]\n",
    "    average = grades.mean()\n",
    "    assert average.mean() == expected"
   ],
   "id": "a835e93e9e0ae24a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting test_upgrade_KT2.py\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-08T13:52:13.042609Z",
     "start_time": "2024-11-08T13:52:09.957311Z"
    }
   },
   "cell_type": "code",
   "source": "!pytest test_upgrade_KT2.py",
   "id": "82f207c0e4d1e589",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m============================= test session starts =============================\u001B[0m\n",
      "platform win32 -- Python 3.9.7, pytest-8.3.3, pluggy-1.5.0\n",
      "rootdir: C:\\Users\\acva0\\Projects\\PycharmProjects\\Pytesting\\KT4\n",
      "plugins: anyio-4.6.2.post1, cov-5.0.0, sugar-1.0.0\n",
      "collected 3 items\n",
      "\n",
      "test_upgrade_KT2.py \u001B[32m.\u001B[0m\u001B[31mF\u001B[0m\u001B[31mF\u001B[0m\u001B[31m                                                  [100%]\u001B[0m\n",
      "\n",
      "================================== FAILURES ===================================\n",
      "\u001B[31m\u001B[1m________________ test_upgrade_average_grade[21-35-42-13-8-20] _________________\u001B[0m\n",
      "\n",
      "test1 = 21, test2 = 35, test3 = 42, test4 = 13, final = 8, expected = 20\n",
      "\n",
      "    \u001B[0m\u001B[37m@pytest\u001B[39;49;00m.mark.parametrize(\u001B[33m\"\u001B[39;49;00m\u001B[33mtest1, test2, test3, test4, final, expected\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m, [(\u001B[94m10\u001B[39;49;00m, \u001B[94m20\u001B[39;49;00m, \u001B[94m30\u001B[39;49;00m, \u001B[94m40\u001B[39;49;00m, \u001B[94m50\u001B[39;49;00m, \u001B[94m30\u001B[39;49;00m), (\u001B[94m21\u001B[39;49;00m, \u001B[94m35\u001B[39;49;00m, \u001B[94m42\u001B[39;49;00m, \u001B[94m13\u001B[39;49;00m, \u001B[94m8\u001B[39;49;00m, \u001B[94m20\u001B[39;49;00m), (\u001B[94m24\u001B[39;49;00m,\u001B[94m68\u001B[39;49;00m, \u001B[94m39\u001B[39;49;00m, \u001B[94m81\u001B[39;49;00m, \u001B[94m97\u001B[39;49;00m, \u001B[94m60\u001B[39;49;00m)])\u001B[90m\u001B[39;49;00m\n",
      "    \u001B[94mdef\u001B[39;49;00m \u001B[92mtest_upgrade_average_grade\u001B[39;49;00m(test1, test2, test3, test4, final, expected):\u001B[90m\u001B[39;49;00m\n",
      "        df = pd.DataFrame({\u001B[33m'\u001B[39;49;00m\u001B[33mTest1\u001B[39;49;00m\u001B[33m'\u001B[39;49;00m: [test1], \u001B[33m'\u001B[39;49;00m\u001B[33mTest2\u001B[39;49;00m\u001B[33m'\u001B[39;49;00m: [test2], \u001B[33m'\u001B[39;49;00m\u001B[33mTest3\u001B[39;49;00m\u001B[33m'\u001B[39;49;00m: [test3], \u001B[33m'\u001B[39;49;00m\u001B[33mTest4\u001B[39;49;00m\u001B[33m'\u001B[39;49;00m: [test4], \u001B[33m'\u001B[39;49;00m\u001B[33mFinal\u001B[39;49;00m\u001B[33m'\u001B[39;49;00m: [final]})\u001B[90m\u001B[39;49;00m\n",
      "        grades = df[[\u001B[33m'\u001B[39;49;00m\u001B[33mTest1\u001B[39;49;00m\u001B[33m'\u001B[39;49;00m, \u001B[33m'\u001B[39;49;00m\u001B[33mTest2\u001B[39;49;00m\u001B[33m'\u001B[39;49;00m, \u001B[33m'\u001B[39;49;00m\u001B[33mTest3\u001B[39;49;00m\u001B[33m'\u001B[39;49;00m, \u001B[33m'\u001B[39;49;00m\u001B[33mTest4\u001B[39;49;00m\u001B[33m'\u001B[39;49;00m, \u001B[33m'\u001B[39;49;00m\u001B[33mFinal\u001B[39;49;00m\u001B[33m'\u001B[39;49;00m]]\u001B[90m\u001B[39;49;00m\n",
      "        average = grades.mean()\u001B[90m\u001B[39;49;00m\n",
      ">       \u001B[94massert\u001B[39;49;00m average.mean() == expected\u001B[90m\u001B[39;49;00m\n",
      "\u001B[1m\u001B[31mE       assert np.float64(23.8) == 20\u001B[0m\n",
      "\u001B[1m\u001B[31mE        +  where np.float64(23.8) = mean()\u001B[0m\n",
      "\u001B[1m\u001B[31mE        +    where mean = Test1    21.0\\nTest2    35.0\\nTest3    42.0\\nTest4    13.0\\nFinal     8.0\\ndtype: float64.mean\u001B[0m\n",
      "\n",
      "\u001B[1m\u001B[31mtest_upgrade_KT2.py\u001B[0m:9: AssertionError\n",
      "\u001B[31m\u001B[1m________________ test_upgrade_average_grade[24-68-39-81-97-60] ________________\u001B[0m\n",
      "\n",
      "test1 = 24, test2 = 68, test3 = 39, test4 = 81, final = 97, expected = 60\n",
      "\n",
      "    \u001B[0m\u001B[37m@pytest\u001B[39;49;00m.mark.parametrize(\u001B[33m\"\u001B[39;49;00m\u001B[33mtest1, test2, test3, test4, final, expected\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m, [(\u001B[94m10\u001B[39;49;00m, \u001B[94m20\u001B[39;49;00m, \u001B[94m30\u001B[39;49;00m, \u001B[94m40\u001B[39;49;00m, \u001B[94m50\u001B[39;49;00m, \u001B[94m30\u001B[39;49;00m), (\u001B[94m21\u001B[39;49;00m, \u001B[94m35\u001B[39;49;00m, \u001B[94m42\u001B[39;49;00m, \u001B[94m13\u001B[39;49;00m, \u001B[94m8\u001B[39;49;00m, \u001B[94m20\u001B[39;49;00m), (\u001B[94m24\u001B[39;49;00m,\u001B[94m68\u001B[39;49;00m, \u001B[94m39\u001B[39;49;00m, \u001B[94m81\u001B[39;49;00m, \u001B[94m97\u001B[39;49;00m, \u001B[94m60\u001B[39;49;00m)])\u001B[90m\u001B[39;49;00m\n",
      "    \u001B[94mdef\u001B[39;49;00m \u001B[92mtest_upgrade_average_grade\u001B[39;49;00m(test1, test2, test3, test4, final, expected):\u001B[90m\u001B[39;49;00m\n",
      "        df = pd.DataFrame({\u001B[33m'\u001B[39;49;00m\u001B[33mTest1\u001B[39;49;00m\u001B[33m'\u001B[39;49;00m: [test1], \u001B[33m'\u001B[39;49;00m\u001B[33mTest2\u001B[39;49;00m\u001B[33m'\u001B[39;49;00m: [test2], \u001B[33m'\u001B[39;49;00m\u001B[33mTest3\u001B[39;49;00m\u001B[33m'\u001B[39;49;00m: [test3], \u001B[33m'\u001B[39;49;00m\u001B[33mTest4\u001B[39;49;00m\u001B[33m'\u001B[39;49;00m: [test4], \u001B[33m'\u001B[39;49;00m\u001B[33mFinal\u001B[39;49;00m\u001B[33m'\u001B[39;49;00m: [final]})\u001B[90m\u001B[39;49;00m\n",
      "        grades = df[[\u001B[33m'\u001B[39;49;00m\u001B[33mTest1\u001B[39;49;00m\u001B[33m'\u001B[39;49;00m, \u001B[33m'\u001B[39;49;00m\u001B[33mTest2\u001B[39;49;00m\u001B[33m'\u001B[39;49;00m, \u001B[33m'\u001B[39;49;00m\u001B[33mTest3\u001B[39;49;00m\u001B[33m'\u001B[39;49;00m, \u001B[33m'\u001B[39;49;00m\u001B[33mTest4\u001B[39;49;00m\u001B[33m'\u001B[39;49;00m, \u001B[33m'\u001B[39;49;00m\u001B[33mFinal\u001B[39;49;00m\u001B[33m'\u001B[39;49;00m]]\u001B[90m\u001B[39;49;00m\n",
      "        average = grades.mean()\u001B[90m\u001B[39;49;00m\n",
      ">       \u001B[94massert\u001B[39;49;00m average.mean() == expected\u001B[90m\u001B[39;49;00m\n",
      "\u001B[1m\u001B[31mE       assert np.float64(61.8) == 60\u001B[0m\n",
      "\u001B[1m\u001B[31mE        +  where np.float64(61.8) = mean()\u001B[0m\n",
      "\u001B[1m\u001B[31mE        +    where mean = Test1    24.0\\nTest2    68.0\\nTest3    39.0\\nTest4    81.0\\nFinal    97.0\\ndtype: float64.mean\u001B[0m\n",
      "\n",
      "\u001B[1m\u001B[31mtest_upgrade_KT2.py\u001B[0m:9: AssertionError\n",
      "\u001B[36m\u001B[1m=========================== short test summary info ===========================\u001B[0m\n",
      "\u001B[31mFAILED\u001B[0m test_upgrade_KT2.py::\u001B[1mtest_upgrade_average_grade[21-35-42-13-8-20]\u001B[0m - assert np.float64(23.8) == 20\n",
      "\u001B[31mFAILED\u001B[0m test_upgrade_KT2.py::\u001B[1mtest_upgrade_average_grade[24-68-39-81-97-60]\u001B[0m - assert np.float64(61.8) == 60\n",
      "\u001B[31m========================= \u001B[31m\u001B[1m2 failed\u001B[0m, \u001B[32m1 passed\u001B[0m\u001B[31m in 2.32s\u001B[0m\u001B[31m =========================\u001B[0m\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Тестим второй апгрейд подсчета среднего",
   "id": "33ba606a21024bc4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-08T14:08:40.168788Z",
     "start_time": "2024-11-08T14:08:40.155809Z"
    }
   },
   "cell_type": "code",
   "source": [
    "%%file upgrade_KT2_2.py\n",
    "import csv\n",
    "import pytest\n",
    "\n",
    "# функция для чтения данных из CSV-файла\n",
    "def load_data_from_csv(file_path):\n",
    "    with open(file_path, encoding=\"utf-8\") as file:\n",
    "        reader = csv.DictReader(file)\n",
    "        data = []\n",
    "        for row in reader:\n",
    "            # Преобразуем оценки и итоговую оценку в числа\n",
    "            test_scores = [float(row['test1']), float(row['test2']), float(row['test3']), float(row['test4'])]\n",
    "            final_grade = float(row['final_grade'])\n",
    "            data.append((row['name'], test_scores, final_grade))\n",
    "        return data\n",
    "\n",
    "# функция для расчета итоговой среднего арифметического\n",
    "def calculate_final_grade(test_scores):\n",
    "    return sum(test_scores) / len(test_scores)\n",
    "    \n",
    "# тест \n",
    "@pytest.mark.parametrize(\"name, test_scores, expected_final_grade\", load_data_from_csv(\"grades.csv\"))\n",
    "def test_student_final_grade(name, test_scores, expected_final_grade):\n",
    "    calculated_final_grade = calculate_final_grade(test_scores)\n",
    "    assert calculated_final_grade == expected_final_grade"
   ],
   "id": "8cc7c0a6e77a7362",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing upgrade_KT2_2.py\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-08T14:08:41.795038Z",
     "start_time": "2024-11-08T14:08:40.748661Z"
    }
   },
   "cell_type": "code",
   "source": "!pytest upgrade_KT2.py",
   "id": "920781a31b0195f4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m============================= test session starts =============================\u001B[0m\n",
      "platform win32 -- Python 3.9.7, pytest-8.3.3, pluggy-1.5.0\n",
      "rootdir: C:\\Users\\acva0\\Projects\\PycharmProjects\\Pytesting\\KT4\n",
      "plugins: anyio-4.6.2.post1, cov-5.0.0, sugar-1.0.0\n",
      "collected 16 items\n",
      "\n",
      "upgrade_KT2.py \u001B[31mF\u001B[0m\u001B[31mF\u001B[0m\u001B[31mF\u001B[0m\u001B[31mF\u001B[0m\u001B[31mF\u001B[0m\u001B[31mF\u001B[0m\u001B[31mF\u001B[0m\u001B[31mF\u001B[0m\u001B[31mF\u001B[0m\u001B[31mF\u001B[0m\u001B[31mF\u001B[0m\u001B[31mF\u001B[0m\u001B[31mF\u001B[0m\u001B[31mF\u001B[0m\u001B[31mF\u001B[0m\u001B[31mF\u001B[0m\u001B[31m                                          [100%]\u001B[0m\n",
      "\n",
      "================================== FAILURES ===================================\n",
      "\u001B[31m\u001B[1m_____________ test_student_final_grade[Alfalfa-test_scores0-49.0] _____________\u001B[0m\n",
      "\n",
      "name = 'Alfalfa', test_scores = [40.0, 90.0, 100.0, 83.0]\n",
      "expected_final_grade = 49.0\n",
      "\n",
      "    \u001B[0m\u001B[37m@pytest\u001B[39;49;00m.mark.parametrize(\u001B[33m\"\u001B[39;49;00m\u001B[33mname, test_scores, expected_final_grade\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m, load_data_from_csv(\u001B[33m\"\u001B[39;49;00m\u001B[33mgrades.csv\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m))\u001B[90m\u001B[39;49;00m\n",
      "    \u001B[94mdef\u001B[39;49;00m \u001B[92mtest_student_final_grade\u001B[39;49;00m(name, test_scores, expected_final_grade):\u001B[90m\u001B[39;49;00m\n",
      "        calculated_final_grade = calculate_final_grade(test_scores)\u001B[90m\u001B[39;49;00m\n",
      ">       \u001B[94massert\u001B[39;49;00m calculated_final_grade == expected_final_grade\u001B[90m\u001B[39;49;00m\n",
      "\u001B[1m\u001B[31mE       assert 78.25 == 49.0\u001B[0m\n",
      "\n",
      "\u001B[1m\u001B[31mupgrade_KT2.py\u001B[0m:25: AssertionError\n",
      "\u001B[31m\u001B[1m_____________ test_student_final_grade[Alfred-test_scores1-48.0] ______________\u001B[0m\n",
      "\n",
      "name = 'Alfred', test_scores = [41.0, 97.0, 96.0, 97.0]\n",
      "expected_final_grade = 48.0\n",
      "\n",
      "    \u001B[0m\u001B[37m@pytest\u001B[39;49;00m.mark.parametrize(\u001B[33m\"\u001B[39;49;00m\u001B[33mname, test_scores, expected_final_grade\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m, load_data_from_csv(\u001B[33m\"\u001B[39;49;00m\u001B[33mgrades.csv\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m))\u001B[90m\u001B[39;49;00m\n",
      "    \u001B[94mdef\u001B[39;49;00m \u001B[92mtest_student_final_grade\u001B[39;49;00m(name, test_scores, expected_final_grade):\u001B[90m\u001B[39;49;00m\n",
      "        calculated_final_grade = calculate_final_grade(test_scores)\u001B[90m\u001B[39;49;00m\n",
      ">       \u001B[94massert\u001B[39;49;00m calculated_final_grade == expected_final_grade\u001B[90m\u001B[39;49;00m\n",
      "\u001B[1m\u001B[31mE       assert 82.75 == 48.0\u001B[0m\n",
      "\n",
      "\u001B[1m\u001B[31mupgrade_KT2.py\u001B[0m:25: AssertionError\n",
      "\u001B[31m\u001B[1m______________ test_student_final_grade[Gerty-test_scores2-44.0] ______________\u001B[0m\n",
      "\n",
      "name = 'Gerty', test_scores = [41.0, 80.0, 60.0, 40.0]\n",
      "expected_final_grade = 44.0\n",
      "\n",
      "    \u001B[0m\u001B[37m@pytest\u001B[39;49;00m.mark.parametrize(\u001B[33m\"\u001B[39;49;00m\u001B[33mname, test_scores, expected_final_grade\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m, load_data_from_csv(\u001B[33m\"\u001B[39;49;00m\u001B[33mgrades.csv\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m))\u001B[90m\u001B[39;49;00m\n",
      "    \u001B[94mdef\u001B[39;49;00m \u001B[92mtest_student_final_grade\u001B[39;49;00m(name, test_scores, expected_final_grade):\u001B[90m\u001B[39;49;00m\n",
      "        calculated_final_grade = calculate_final_grade(test_scores)\u001B[90m\u001B[39;49;00m\n",
      ">       \u001B[94massert\u001B[39;49;00m calculated_final_grade == expected_final_grade\u001B[90m\u001B[39;49;00m\n",
      "\u001B[1m\u001B[31mE       assert 55.25 == 44.0\u001B[0m\n",
      "\n",
      "\u001B[1m\u001B[31mupgrade_KT2.py\u001B[0m:25: AssertionError\n",
      "\u001B[31m\u001B[1m_____________ test_student_final_grade[Android-test_scores3-47.0] _____________\u001B[0m\n",
      "\n",
      "name = 'Android', test_scores = [42.0, 23.0, 36.0, 45.0]\n",
      "expected_final_grade = 47.0\n",
      "\n",
      "    \u001B[0m\u001B[37m@pytest\u001B[39;49;00m.mark.parametrize(\u001B[33m\"\u001B[39;49;00m\u001B[33mname, test_scores, expected_final_grade\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m, load_data_from_csv(\u001B[33m\"\u001B[39;49;00m\u001B[33mgrades.csv\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m))\u001B[90m\u001B[39;49;00m\n",
      "    \u001B[94mdef\u001B[39;49;00m \u001B[92mtest_student_final_grade\u001B[39;49;00m(name, test_scores, expected_final_grade):\u001B[90m\u001B[39;49;00m\n",
      "        calculated_final_grade = calculate_final_grade(test_scores)\u001B[90m\u001B[39;49;00m\n",
      ">       \u001B[94massert\u001B[39;49;00m calculated_final_grade == expected_final_grade\u001B[90m\u001B[39;49;00m\n",
      "\u001B[1m\u001B[31mE       assert 36.5 == 47.0\u001B[0m\n",
      "\n",
      "\u001B[1m\u001B[31mupgrade_KT2.py\u001B[0m:25: AssertionError\n",
      "\u001B[31m\u001B[1m_____________ test_student_final_grade[Bumpkin-test_scores4-45.0] _____________\u001B[0m\n",
      "\n",
      "name = 'Bumpkin', test_scores = [43.0, 78.0, 88.0, 77.0]\n",
      "expected_final_grade = 45.0\n",
      "\n",
      "    \u001B[0m\u001B[37m@pytest\u001B[39;49;00m.mark.parametrize(\u001B[33m\"\u001B[39;49;00m\u001B[33mname, test_scores, expected_final_grade\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m, load_data_from_csv(\u001B[33m\"\u001B[39;49;00m\u001B[33mgrades.csv\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m))\u001B[90m\u001B[39;49;00m\n",
      "    \u001B[94mdef\u001B[39;49;00m \u001B[92mtest_student_final_grade\u001B[39;49;00m(name, test_scores, expected_final_grade):\u001B[90m\u001B[39;49;00m\n",
      "        calculated_final_grade = calculate_final_grade(test_scores)\u001B[90m\u001B[39;49;00m\n",
      ">       \u001B[94massert\u001B[39;49;00m calculated_final_grade == expected_final_grade\u001B[90m\u001B[39;49;00m\n",
      "\u001B[1m\u001B[31mE       assert 71.5 == 45.0\u001B[0m\n",
      "\n",
      "\u001B[1m\u001B[31mupgrade_KT2.py\u001B[0m:25: AssertionError\n",
      "\u001B[31m\u001B[1m_____________ test_student_final_grade[Rubble-test_scores5-46.0] ______________\u001B[0m\n",
      "\n",
      "name = 'Rubble', test_scores = [44.0, 90.0, 80.0, 90.0]\n",
      "expected_final_grade = 46.0\n",
      "\n",
      "    \u001B[0m\u001B[37m@pytest\u001B[39;49;00m.mark.parametrize(\u001B[33m\"\u001B[39;49;00m\u001B[33mname, test_scores, expected_final_grade\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m, load_data_from_csv(\u001B[33m\"\u001B[39;49;00m\u001B[33mgrades.csv\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m))\u001B[90m\u001B[39;49;00m\n",
      "    \u001B[94mdef\u001B[39;49;00m \u001B[92mtest_student_final_grade\u001B[39;49;00m(name, test_scores, expected_final_grade):\u001B[90m\u001B[39;49;00m\n",
      "        calculated_final_grade = calculate_final_grade(test_scores)\u001B[90m\u001B[39;49;00m\n",
      ">       \u001B[94massert\u001B[39;49;00m calculated_final_grade == expected_final_grade\u001B[90m\u001B[39;49;00m\n",
      "\u001B[1m\u001B[31mE       assert 76.0 == 46.0\u001B[0m\n",
      "\n",
      "\u001B[1m\u001B[31mupgrade_KT2.py\u001B[0m:25: AssertionError\n",
      "\u001B[31m\u001B[1m_____________ test_student_final_grade[Noshow-test_scores6-43.0] ______________\u001B[0m\n",
      "\n",
      "name = 'Noshow', test_scores = [45.0, 11.0, -1.0, 4.0]\n",
      "expected_final_grade = 43.0\n",
      "\n",
      "    \u001B[0m\u001B[37m@pytest\u001B[39;49;00m.mark.parametrize(\u001B[33m\"\u001B[39;49;00m\u001B[33mname, test_scores, expected_final_grade\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m, load_data_from_csv(\u001B[33m\"\u001B[39;49;00m\u001B[33mgrades.csv\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m))\u001B[90m\u001B[39;49;00m\n",
      "    \u001B[94mdef\u001B[39;49;00m \u001B[92mtest_student_final_grade\u001B[39;49;00m(name, test_scores, expected_final_grade):\u001B[90m\u001B[39;49;00m\n",
      "        calculated_final_grade = calculate_final_grade(test_scores)\u001B[90m\u001B[39;49;00m\n",
      ">       \u001B[94massert\u001B[39;49;00m calculated_final_grade == expected_final_grade\u001B[90m\u001B[39;49;00m\n",
      "\u001B[1m\u001B[31mE       assert 14.75 == 43.0\u001B[0m\n",
      "\n",
      "\u001B[1m\u001B[31mupgrade_KT2.py\u001B[0m:25: AssertionError\n",
      "\u001B[31m\u001B[1m______________ test_student_final_grade[Buff-test_scores7-50.0] _______________\u001B[0m\n",
      "\n",
      "name = 'Buff', test_scores = [46.0, 20.0, 30.0, 40.0]\n",
      "expected_final_grade = 50.0\n",
      "\n",
      "    \u001B[0m\u001B[37m@pytest\u001B[39;49;00m.mark.parametrize(\u001B[33m\"\u001B[39;49;00m\u001B[33mname, test_scores, expected_final_grade\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m, load_data_from_csv(\u001B[33m\"\u001B[39;49;00m\u001B[33mgrades.csv\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m))\u001B[90m\u001B[39;49;00m\n",
      "    \u001B[94mdef\u001B[39;49;00m \u001B[92mtest_student_final_grade\u001B[39;49;00m(name, test_scores, expected_final_grade):\u001B[90m\u001B[39;49;00m\n",
      "        calculated_final_grade = calculate_final_grade(test_scores)\u001B[90m\u001B[39;49;00m\n",
      ">       \u001B[94massert\u001B[39;49;00m calculated_final_grade == expected_final_grade\u001B[90m\u001B[39;49;00m\n",
      "\u001B[1m\u001B[31mE       assert 34.0 == 50.0\u001B[0m\n",
      "\n",
      "\u001B[1m\u001B[31mupgrade_KT2.py\u001B[0m:25: AssertionError\n",
      "\u001B[31m\u001B[1m_____________ test_student_final_grade[Airpump-test_scores8-83.0] _____________\u001B[0m\n",
      "\n",
      "name = 'Airpump', test_scores = [49.0, 1.0, 90.0, 100.0]\n",
      "expected_final_grade = 83.0\n",
      "\n",
      "    \u001B[0m\u001B[37m@pytest\u001B[39;49;00m.mark.parametrize(\u001B[33m\"\u001B[39;49;00m\u001B[33mname, test_scores, expected_final_grade\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m, load_data_from_csv(\u001B[33m\"\u001B[39;49;00m\u001B[33mgrades.csv\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m))\u001B[90m\u001B[39;49;00m\n",
      "    \u001B[94mdef\u001B[39;49;00m \u001B[92mtest_student_final_grade\u001B[39;49;00m(name, test_scores, expected_final_grade):\u001B[90m\u001B[39;49;00m\n",
      "        calculated_final_grade = calculate_final_grade(test_scores)\u001B[90m\u001B[39;49;00m\n",
      ">       \u001B[94massert\u001B[39;49;00m calculated_final_grade == expected_final_grade\u001B[90m\u001B[39;49;00m\n",
      "\u001B[1m\u001B[31mE       assert 60.0 == 83.0\u001B[0m\n",
      "\n",
      "\u001B[1m\u001B[31mupgrade_KT2.py\u001B[0m:25: AssertionError\n",
      "\u001B[31m\u001B[1m_____________ test_student_final_grade[Backus-test_scores9-97.0] ______________\u001B[0m\n",
      "\n",
      "name = 'Backus', test_scores = [48.0, 1.0, 97.0, 96.0]\n",
      "expected_final_grade = 97.0\n",
      "\n",
      "    \u001B[0m\u001B[37m@pytest\u001B[39;49;00m.mark.parametrize(\u001B[33m\"\u001B[39;49;00m\u001B[33mname, test_scores, expected_final_grade\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m, load_data_from_csv(\u001B[33m\"\u001B[39;49;00m\u001B[33mgrades.csv\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m))\u001B[90m\u001B[39;49;00m\n",
      "    \u001B[94mdef\u001B[39;49;00m \u001B[92mtest_student_final_grade\u001B[39;49;00m(name, test_scores, expected_final_grade):\u001B[90m\u001B[39;49;00m\n",
      "        calculated_final_grade = calculate_final_grade(test_scores)\u001B[90m\u001B[39;49;00m\n",
      ">       \u001B[94massert\u001B[39;49;00m calculated_final_grade == expected_final_grade\u001B[90m\u001B[39;49;00m\n",
      "\u001B[1m\u001B[31mE       assert 60.5 == 97.0\u001B[0m\n",
      "\n",
      "\u001B[1m\u001B[31mupgrade_KT2.py\u001B[0m:25: AssertionError\n",
      "\u001B[31m\u001B[1m___________ test_student_final_grade[Carnivore-test_scores10-40.0] ____________\u001B[0m\n",
      "\n",
      "name = 'Carnivore', test_scores = [44.0, 1.0, 80.0, 60.0]\n",
      "expected_final_grade = 40.0\n",
      "\n",
      "    \u001B[0m\u001B[37m@pytest\u001B[39;49;00m.mark.parametrize(\u001B[33m\"\u001B[39;49;00m\u001B[33mname, test_scores, expected_final_grade\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m, load_data_from_csv(\u001B[33m\"\u001B[39;49;00m\u001B[33mgrades.csv\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m))\u001B[90m\u001B[39;49;00m\n",
      "    \u001B[94mdef\u001B[39;49;00m \u001B[92mtest_student_final_grade\u001B[39;49;00m(name, test_scores, expected_final_grade):\u001B[90m\u001B[39;49;00m\n",
      "        calculated_final_grade = calculate_final_grade(test_scores)\u001B[90m\u001B[39;49;00m\n",
      ">       \u001B[94massert\u001B[39;49;00m calculated_final_grade == expected_final_grade\u001B[90m\u001B[39;49;00m\n",
      "\u001B[1m\u001B[31mE       assert 46.25 == 40.0\u001B[0m\n",
      "\n",
      "\u001B[1m\u001B[31mupgrade_KT2.py\u001B[0m:25: AssertionError\n",
      "\u001B[31m\u001B[1m_____________ test_student_final_grade[Dandy-test_scores11-45.0] ______________\u001B[0m\n",
      "\n",
      "name = 'Dandy', test_scores = [47.0, 1.0, 23.0, 36.0]\n",
      "expected_final_grade = 45.0\n",
      "\n",
      "    \u001B[0m\u001B[37m@pytest\u001B[39;49;00m.mark.parametrize(\u001B[33m\"\u001B[39;49;00m\u001B[33mname, test_scores, expected_final_grade\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m, load_data_from_csv(\u001B[33m\"\u001B[39;49;00m\u001B[33mgrades.csv\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m))\u001B[90m\u001B[39;49;00m\n",
      "    \u001B[94mdef\u001B[39;49;00m \u001B[92mtest_student_final_grade\u001B[39;49;00m(name, test_scores, expected_final_grade):\u001B[90m\u001B[39;49;00m\n",
      "        calculated_final_grade = calculate_final_grade(test_scores)\u001B[90m\u001B[39;49;00m\n",
      ">       \u001B[94massert\u001B[39;49;00m calculated_final_grade == expected_final_grade\u001B[90m\u001B[39;49;00m\n",
      "\u001B[1m\u001B[31mE       assert 26.75 == 45.0\u001B[0m\n",
      "\n",
      "\u001B[1m\u001B[31mupgrade_KT2.py\u001B[0m:25: AssertionError\n",
      "\u001B[31m\u001B[1m____________ test_student_final_grade[Elephant-test_scores12-77.0] ____________\u001B[0m\n",
      "\n",
      "name = 'Elephant', test_scores = [45.0, 1.0, 78.0, 88.0]\n",
      "expected_final_grade = 77.0\n",
      "\n",
      "    \u001B[0m\u001B[37m@pytest\u001B[39;49;00m.mark.parametrize(\u001B[33m\"\u001B[39;49;00m\u001B[33mname, test_scores, expected_final_grade\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m, load_data_from_csv(\u001B[33m\"\u001B[39;49;00m\u001B[33mgrades.csv\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m))\u001B[90m\u001B[39;49;00m\n",
      "    \u001B[94mdef\u001B[39;49;00m \u001B[92mtest_student_final_grade\u001B[39;49;00m(name, test_scores, expected_final_grade):\u001B[90m\u001B[39;49;00m\n",
      "        calculated_final_grade = calculate_final_grade(test_scores)\u001B[90m\u001B[39;49;00m\n",
      ">       \u001B[94massert\u001B[39;49;00m calculated_final_grade == expected_final_grade\u001B[90m\u001B[39;49;00m\n",
      "\u001B[1m\u001B[31mE       assert 53.0 == 77.0\u001B[0m\n",
      "\n",
      "\u001B[1m\u001B[31mupgrade_KT2.py\u001B[0m:25: AssertionError\n",
      "\u001B[31m\u001B[1m____________ test_student_final_grade[Franklin-test_scores13-90.0] ____________\u001B[0m\n",
      "\n",
      "name = 'Franklin', test_scores = [50.0, 1.0, 90.0, 80.0]\n",
      "expected_final_grade = 90.0\n",
      "\n",
      "    \u001B[0m\u001B[37m@pytest\u001B[39;49;00m.mark.parametrize(\u001B[33m\"\u001B[39;49;00m\u001B[33mname, test_scores, expected_final_grade\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m, load_data_from_csv(\u001B[33m\"\u001B[39;49;00m\u001B[33mgrades.csv\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m))\u001B[90m\u001B[39;49;00m\n",
      "    \u001B[94mdef\u001B[39;49;00m \u001B[92mtest_student_final_grade\u001B[39;49;00m(name, test_scores, expected_final_grade):\u001B[90m\u001B[39;49;00m\n",
      "        calculated_final_grade = calculate_final_grade(test_scores)\u001B[90m\u001B[39;49;00m\n",
      ">       \u001B[94massert\u001B[39;49;00m calculated_final_grade == expected_final_grade\u001B[90m\u001B[39;49;00m\n",
      "\u001B[1m\u001B[31mE       assert 55.25 == 90.0\u001B[0m\n",
      "\n",
      "\u001B[1m\u001B[31mupgrade_KT2.py\u001B[0m:25: AssertionError\n",
      "\u001B[31m\u001B[1m_____________ test_student_final_grade[George-test_scores14-4.0] ______________\u001B[0m\n",
      "\n",
      "name = 'George', test_scores = [40.0, 1.0, 11.0, -1.0]\n",
      "expected_final_grade = 4.0\n",
      "\n",
      "    \u001B[0m\u001B[37m@pytest\u001B[39;49;00m.mark.parametrize(\u001B[33m\"\u001B[39;49;00m\u001B[33mname, test_scores, expected_final_grade\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m, load_data_from_csv(\u001B[33m\"\u001B[39;49;00m\u001B[33mgrades.csv\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m))\u001B[90m\u001B[39;49;00m\n",
      "    \u001B[94mdef\u001B[39;49;00m \u001B[92mtest_student_final_grade\u001B[39;49;00m(name, test_scores, expected_final_grade):\u001B[90m\u001B[39;49;00m\n",
      "        calculated_final_grade = calculate_final_grade(test_scores)\u001B[90m\u001B[39;49;00m\n",
      ">       \u001B[94massert\u001B[39;49;00m calculated_final_grade == expected_final_grade\u001B[90m\u001B[39;49;00m\n",
      "\u001B[1m\u001B[31mE       assert 12.75 == 4.0\u001B[0m\n",
      "\n",
      "\u001B[1m\u001B[31mupgrade_KT2.py\u001B[0m:25: AssertionError\n",
      "\u001B[31m\u001B[1m___________ test_student_final_grade[Heffalump-test_scores15-40.0] ____________\u001B[0m\n",
      "\n",
      "name = 'Heffalump', test_scores = [30.0, 1.0, 20.0, 30.0]\n",
      "expected_final_grade = 40.0\n",
      "\n",
      "    \u001B[0m\u001B[37m@pytest\u001B[39;49;00m.mark.parametrize(\u001B[33m\"\u001B[39;49;00m\u001B[33mname, test_scores, expected_final_grade\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m, load_data_from_csv(\u001B[33m\"\u001B[39;49;00m\u001B[33mgrades.csv\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m))\u001B[90m\u001B[39;49;00m\n",
      "    \u001B[94mdef\u001B[39;49;00m \u001B[92mtest_student_final_grade\u001B[39;49;00m(name, test_scores, expected_final_grade):\u001B[90m\u001B[39;49;00m\n",
      "        calculated_final_grade = calculate_final_grade(test_scores)\u001B[90m\u001B[39;49;00m\n",
      ">       \u001B[94massert\u001B[39;49;00m calculated_final_grade == expected_final_grade\u001B[90m\u001B[39;49;00m\n",
      "\u001B[1m\u001B[31mE       assert 20.25 == 40.0\u001B[0m\n",
      "\n",
      "\u001B[1m\u001B[31mupgrade_KT2.py\u001B[0m:25: AssertionError\n",
      "\u001B[36m\u001B[1m=========================== short test summary info ===========================\u001B[0m\n",
      "\u001B[31mFAILED\u001B[0m upgrade_KT2.py::\u001B[1mtest_student_final_grade[Alfalfa-test_scores0-49.0]\u001B[0m - assert 78.25 == 49.0\n",
      "\u001B[31mFAILED\u001B[0m upgrade_KT2.py::\u001B[1mtest_student_final_grade[Alfred-test_scores1-48.0]\u001B[0m - assert 82.75 == 48.0\n",
      "\u001B[31mFAILED\u001B[0m upgrade_KT2.py::\u001B[1mtest_student_final_grade[Gerty-test_scores2-44.0]\u001B[0m - assert 55.25 == 44.0\n",
      "\u001B[31mFAILED\u001B[0m upgrade_KT2.py::\u001B[1mtest_student_final_grade[Android-test_scores3-47.0]\u001B[0m - assert 36.5 == 47.0\n",
      "\u001B[31mFAILED\u001B[0m upgrade_KT2.py::\u001B[1mtest_student_final_grade[Bumpkin-test_scores4-45.0]\u001B[0m - assert 71.5 == 45.0\n",
      "\u001B[31mFAILED\u001B[0m upgrade_KT2.py::\u001B[1mtest_student_final_grade[Rubble-test_scores5-46.0]\u001B[0m - assert 76.0 == 46.0\n",
      "\u001B[31mFAILED\u001B[0m upgrade_KT2.py::\u001B[1mtest_student_final_grade[Noshow-test_scores6-43.0]\u001B[0m - assert 14.75 == 43.0\n",
      "\u001B[31mFAILED\u001B[0m upgrade_KT2.py::\u001B[1mtest_student_final_grade[Buff-test_scores7-50.0]\u001B[0m - assert 34.0 == 50.0\n",
      "\u001B[31mFAILED\u001B[0m upgrade_KT2.py::\u001B[1mtest_student_final_grade[Airpump-test_scores8-83.0]\u001B[0m - assert 60.0 == 83.0\n",
      "\u001B[31mFAILED\u001B[0m upgrade_KT2.py::\u001B[1mtest_student_final_grade[Backus-test_scores9-97.0]\u001B[0m - assert 60.5 == 97.0\n",
      "\u001B[31mFAILED\u001B[0m upgrade_KT2.py::\u001B[1mtest_student_final_grade[Carnivore-test_scores10-40.0]\u001B[0m - assert 46.25 == 40.0\n",
      "\u001B[31mFAILED\u001B[0m upgrade_KT2.py::\u001B[1mtest_student_final_grade[Dandy-test_scores11-45.0]\u001B[0m - assert 26.75 == 45.0\n",
      "\u001B[31mFAILED\u001B[0m upgrade_KT2.py::\u001B[1mtest_student_final_grade[Elephant-test_scores12-77.0]\u001B[0m - assert 53.0 == 77.0\n",
      "\u001B[31mFAILED\u001B[0m upgrade_KT2.py::\u001B[1mtest_student_final_grade[Franklin-test_scores13-90.0]\u001B[0m - assert 55.25 == 90.0\n",
      "\u001B[31mFAILED\u001B[0m upgrade_KT2.py::\u001B[1mtest_student_final_grade[George-test_scores14-4.0]\u001B[0m - assert 12.75 == 4.0\n",
      "\u001B[31mFAILED\u001B[0m upgrade_KT2.py::\u001B[1mtest_student_final_grade[Heffalump-test_scores15-40.0]\u001B[0m - assert 20.25 == 40.0\n",
      "\u001B[31m============================= \u001B[31m\u001B[1m16 failed\u001B[0m\u001B[31m in 0.21s\u001B[0m\u001B[31m ==============================\u001B[0m\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "b86dce638d7f728"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
